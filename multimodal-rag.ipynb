{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d9f14-2861-479a-aecd-d6d428d36b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############# LOADER ##############\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe5953-df7f-413f-9e0e-10878c51c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://sgp.fas.org/crs/misc/IF10244.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245db62-01dd-48b9-a611-97e3abdc8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from unstructured.partition.utils.constants import PartitionStrategy\n",
    "\n",
    "\n",
    "loader = UnstructuredPDFLoader(\n",
    "    file_path=\"./IF10244.pdf\",\n",
    "    strategy=PartitionStrategy.HI_RES,\n",
    "    infer_table_structure=True,\n",
    "    extract_images_in_pdf=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    new_after_n_chars=4000,  # Soft-max\n",
    "    max_characters=4000,  # Hard-max\n",
    "    combine_text_under_n_chars=2000,  # Combine chunks of < 200 chars\n",
    "    mode='elements',  # Split the documents into elements such as Title and NarrativeText.\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1d07f-e629-4242-9d82-4cd7313030e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec031b9-9114-4ad9-ad58-4627b90992f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d60108-3725-45e4-b9b3-aa3fb9b85bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac55078-6db4-44a0-8171-142529d529a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].metadata[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c19ff6-0e43-40a3-92ec-0c66890b73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc.metadata['category'] for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e12e11-80e0-494c-b694-cd63ebab9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2]  # Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099f064-6cf9-4e26-8bf9-a6c8c606bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a940ad-07d7-4f36-980f-a5e0d019a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2].metadata['text_as_html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2b39c-d9bc-415f-bcae-ae259c2aa324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(data[2].metadata['text_as_html']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dec5ad-4626-4021-a48e-66f76c98f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data into text and tables list\n",
    "from htmltabletomd import convert_table\n",
    "\n",
    "text, tables = [], []\n",
    "\n",
    "for doc in data:\n",
    "    if doc.metadata['category'] == 'CompositeElement':\n",
    "        text.append(doc)\n",
    "    elif doc.metadata['category'] == 'Table':\n",
    "        doc.page_content = convert_table(doc.metadata['text_as_html'])\n",
    "        tables.append(doc)\n",
    "\n",
    "print(f\"Total number of text records: {len(text)}\")\n",
    "print(f\"Total number of table records: {len(tables)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea74b5-77d5-4a4a-96c6-6559ca5e0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(tables[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc21ac-9b81-4868-a844-f73be09456bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1157235-2e36-473f-a224-3c0fd94b9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltrh ./figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf9c82-b0bb-495f-94b5-f5d096edd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "display(Image('./figures/figure-1-2.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288694bd-b1e1-47d6-b658-ff2c7522500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############ SUMMARY ##############\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a9618-3c61-4381-803c-ef301b88ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass('Enter OpenAI Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9898e88-9eb7-4bba-8e03-c19779a3f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "CHAT_MODEL = ChatOpenAI(model_name='gpt-4o-mini', api_key=OPENAI_API_KEY, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101de3b-4072-482a-b6f3-26d8d804d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize TEXT and TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c843f-4fbc-4b6a-9de3-e0e170173d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "summarize_prompt_text = \"\"\"\n",
    "You are an assistant summarizing tables or text for retrieval purposes. Create a concise, detailed summary \n",
    "optimized for retrieval. If it's a table, include a brief description of its content along with the summary. \n",
    "Do not add extra labels like \"Summary.\"\n",
    "\n",
    "Content to summarize:\n",
    "{context}\n",
    "\"\"\"\n",
    "summarize_prompt = ChatPromptTemplate.from_template(summarize_prompt_text)\n",
    "\n",
    "summarize_chain = {\"context\": RunnablePassthrough()} | summarize_prompt | CHAT_MODEL | StrOutputParser()\n",
    "\n",
    "text_docs = [txt.page_content for txt in text]\n",
    "table_docs = [table.page_content for table in tables]\n",
    "\n",
    "text_summaries = summarize_chain.batch(text_docs, {\"max_concurrency\": 5})\n",
    "table_summaries = summarize_chain.batch(table_docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed1c17-4949-41bd-ae9b-5a56007a2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2db24-44df-48cf-8321-aa737b0d2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cebea-325b-46eb-a03c-930ed0cbf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424e69c-f9f5-49de-9943-fbc5a5619d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def summarize_image(base64_image, prompt_text):\n",
    "    response = CHAT_MODEL.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "def generate_image_summary(image_folder):\n",
    "    base64_images = []\n",
    "    summaries = []\n",
    "\n",
    "    summary_prompt = \"\"\"\n",
    "    You are tasked with summarizing images for retrieval. \n",
    "    The images may contain charts, tables, or graphs. \n",
    "    Create a detailed summary optimized for retrieval without extra labels like 'Summary:'\n",
    "    \"\"\"\n",
    "    \n",
    "    for image in sorted(os.listdir(image_folder)):\n",
    "        if image.endswith(\".jpg\"):\n",
    "            full_image_path = os.path.join(image_folder, image)\n",
    "            encoded_image = image_to_base64(full_image_path)\n",
    "            base64_images.append(encoded_image)\n",
    "            summaries.append(summarize_image(encoded_image, summary_prompt))\n",
    "    \n",
    "    return base64_images, summaries\n",
    "\n",
    "encoded_images, img_summaries = generate_image_summary('./figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a31e1-a99b-404d-81bc-78cb73e4ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14e75f-485d-4069-9a0d-7581f34623dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('./figures/figure-1-2.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c97e7-1535-49ba-8a84-c2ca4df1b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "########### RETRIEVER #############\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5863f1-d943-484d-a6f1-bc2b3ad6f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "OPENAI_EMBEDDING_MODEL = OpenAIEmbeddings(model='text-embedding-ada-002', api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18691e26-e616-40fa-a92e-b8c1d4c6f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name='OSM-21-Oct-2024',\n",
    "    embedding_function=OPENAI_EMBEDDING_MODEL,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "docstore = RedisStore(client=get_client('redis://localhost:6379'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cdecc-616b-4f86-9524-ecda69932a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e627dcc-a9e9-4dfe-907c-9fac55d99af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def add_documents(retriever, summaries, contents):\n",
    "    # Generate unique IDs for each document\n",
    "    document_ids = [str(uuid.uuid4()) for _ in contents]\n",
    "    \n",
    "    # Create documents using summaries and their associated IDs\n",
    "    summarized_docs = [\n",
    "        Document(page_content=summary, metadata={retriever.id_key: document_ids[idx]})\n",
    "        for idx, summary in enumerate(summaries)\n",
    "    ]\n",
    "    \n",
    "    # Add summarized documents to the vector store\n",
    "    retriever.vectorstore.add_documents(summarized_docs)\n",
    "    \n",
    "    # Map document IDs to their full contents in the docstore\n",
    "    retriever.docstore.mset(list(zip(document_ids, contents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c2f2d-4079-4b7d-bba3-0b12d646aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(retriever, text_summaries, text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b858c-1bc5-4e8b-9ed1-8164d3091594",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(retriever, table_summaries, table_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8f789-012c-4dfa-b77f-e9ae385ba847",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(retriever, img_summaries, encoded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f99b9-00e5-4de7-8719-39ec651ca417",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497b00a-d0d0-484a-9edb-f8752c4e1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82443052-633d-48ff-8212-e2ff765f980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Test Multi-Vector Retriever #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56244e64-dd69-4ddf-aeee-972e478bd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which year has the highest acres burned?\"\n",
    "docs = retriever.invoke(query, limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4876834-59cb-43cb-a883-116880649c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs), docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeecea4-e1a3-4077-9035-14e9b4bc65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Image\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def display_base64_image(img_base64):\n",
    "    # Decode the base64 string\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    # Create a BytesIO object\n",
    "    img_buffer = BytesIO(img_data)\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_buffer)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d24eb5-0a29-4d6e-8624-59d8d401f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_base64_image(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344c693-3690-4c7b-8fd6-2d588c3133ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "########### Synthesis #############\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b09c3-e2d1-40ac-99ce-85a5ee1838f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "\n",
    "def is_base64_encoded(string):\n",
    "    \"\"\"Determine if the input string resembles base64 encoding.\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", string) is not None\n",
    "\n",
    "def is_valid_image_data(encoded_data):\n",
    "    \"\"\"\n",
    "    Verify if the base64 data corresponds to an image by checking its initial bytes.\n",
    "    \"\"\"\n",
    "    image_headers = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        decoded_bytes = base64.b64decode(encoded_data)[:8]  # Decode and check the first 8 bytes\n",
    "        return any(decoded_bytes.startswith(header) for header in image_headers)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def categorize_images_and_texts(documents):\n",
    "    \"\"\"\n",
    "    Separate base64-encoded images and text elements (including tables) from documents.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    text_elements = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Check if it's a Document object and extract page content\n",
    "        content = doc.page_content.decode('utf-8') if isinstance(doc, Document) else doc.decode('utf-8')\n",
    "        \n",
    "        # Classify as image or text\n",
    "        if is_base64_encoded(content) and is_valid_image_data(content):\n",
    "            images.append(content)\n",
    "        else:\n",
    "            text_elements.append(content)\n",
    "    \n",
    "    return {\"images\": images, \"texts\": text_elements}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3d77f-4998-4d3d-8319-69d0d8d7fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def generate_multimodal_prompt(data):\n",
    "    # Format text data\n",
    "    combined_texts = \"\\n\".join(data[\"context\"][\"texts\"])\n",
    "    message_list = []\n",
    "    \n",
    "    # If images are present, add them to the messages\n",
    "    if data[\"context\"][\"images\"]:\n",
    "        for img in data[\"context\"][\"images\"]:\n",
    "            message_list.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img}\"}\n",
    "            })\n",
    "    \n",
    "    # Add the text content for analysis\n",
    "    message_list.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            f\"\"\"You are an analyst tasked with interpreting detailed information \n",
    "                and identifying trends from text documents, tables, and charts or graphs in images. \n",
    "                Below is the context, which includes a combination of text, tables, and images, often in the form of charts or graphs.\n",
    "                Use this data to answer the user’s question without fabricating information. \n",
    "                Rely on the given context to respond accurately.\n",
    "                \n",
    "                User question:\n",
    "                {data['question']}\n",
    "                \n",
    "                Context documents:\n",
    "                {combined_texts}\n",
    "                \n",
    "                Answer:\n",
    "            \"\"\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    return [HumanMessage(content=message_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e5e33-e08a-47b9-b4f1-8a3d6f0b8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_rag_chain = (\n",
    "    {\"context\": itemgetter('context'), \"question\": itemgetter('input')}\n",
    "    | RunnableLambda(generate_multimodal_prompt)\n",
    "    | CHAT_MODEL\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "document_retriever = itemgetter('input') | retriever | RunnableLambda(categorize_images_and_texts)\n",
    "\n",
    "\n",
    "mm_rag = (\n",
    "    RunnablePassthrough.assign(context=document_retriever).assign(answer=multimodal_rag_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406efd4c-6a71-4ebe-b001-352d6305808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which year has the highest acres burned?\"\n",
    "response = mm_rag.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcda3c-f8b8-4a31-a60e-1cf300042948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(resp):\n",
    "    display(Markdown(\"#### Input\"))\n",
    "    print(resp['input'])\n",
    "    display(Markdown(\"#### Output\"))\n",
    "    print(resp['answer'])\n",
    "    display(Markdown(\"#### Source Documents\"))\n",
    "    display(Markdown(\"##### Text & Tables\"))\n",
    "    print('\\n'.join(resp['context'].get('texts', [])))\n",
    "    display(Markdown(\"##### Images\"))\n",
    "    for img in resp['context'].get('images', []):\n",
    "        display_base64_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18ba7a-5ce8-4ac0-b731-5f728d98a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673c2df-833e-44a4-a25f-bf0a91c011b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me the number of fires and the acres burned for the department of the interior in the year 2020\"\n",
    "response = mm_rag.invoke({\"input\": query})\n",
    "format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84258156-7a82-405f-8258-de33891ff8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a621c-b9e6-4c77-8af4-c03adc9360b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the number of acres burned by wildfires for the forest service in 2021\"\n",
    "response = mm_rag.invoke({\"input\": query})\n",
    "format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd901ee-7bb1-45af-b62f-e8ab78714614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
